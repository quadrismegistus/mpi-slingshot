{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions available for slingshot\n",
    "STONES = ['ner_spacy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_spacy_string(string):\n",
    "    \"\"\"\n",
    "    Using spacy, this function takes any string, identifies the named entities in it,\n",
    "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
    "    where each dictionary looks like this:\n",
    "    \n",
    "    {\n",
    "        'type': 'PERSON',\n",
    "        'entity': 'Ryan',\n",
    "        '_sent_num': 1,\n",
    "        '_sent': 'Ryan Heuser cannot wait until he graduates from Stanford University.'\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # import spacy\n",
    "        import spacy\n",
    "        import nltk\n",
    "    except ImportError:\n",
    "        print(\"spacy not installed. Please follow directions above.\")\n",
    "        return\n",
    "\n",
    "    # clean string\n",
    "    string = string.strip().replace('\\n',' ').replace(\"’\",\"'\").replace(\"‘\",\"'\")\n",
    "    \n",
    "    # load its default English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # make an output list\n",
    "    output_list = []\n",
    "    \n",
    "    # split at pargraphs:\n",
    "    #paragraphs=string.split('\\n\\n')\n",
    "    #for para_i,para in enumerate(paragraphs):\n",
    "    #    if not para_i%10: print(para_i,'of',len(paragraphs),'paragraphs')\n",
    "    \n",
    "    # create a spacy text object\n",
    "    #doc = nlp(string,disable=['parser','tagger','ner'])\n",
    "\n",
    "    # loop over sentences\n",
    "    sent_num=0\n",
    "    #for sent in doc.sents:\n",
    "    sents=nltk.sent_tokenize(string)\n",
    "    for sent in sents:\n",
    "        sent_doc=nlp(sent, disable=['parser','tagger'])\n",
    "        if not sent_num%1000: print(sent_num,len(sents))\n",
    "        \n",
    "        sent_num+=1\n",
    "        added_sent_already = False\n",
    "\n",
    "        # loop over sentence's entities\n",
    "        #sent_doc = nlp(str(sent))\n",
    "        for ent in sent_doc.ents:\n",
    "\n",
    "            # make a result dict\n",
    "            result_dict = {}\n",
    "\n",
    "            # set sentence number\n",
    "            #result_dict['_para_num'] = para_i+1\n",
    "            result_dict['_sent_num'] = sent_num\n",
    "\n",
    "            # store text too\n",
    "            if not added_sent_already:\n",
    "                result_dict['_sent'] = sent\n",
    "                added_sent_already = True\n",
    "            else:\n",
    "                result_dict['_sent'] = ''\n",
    "\n",
    "            # get type\n",
    "            result_dict['type'] = ent.label_\n",
    "\n",
    "            # get entity\n",
    "            result_dict['entity'] = ent.text\n",
    "\n",
    "            # get start char\n",
    "            result_dict['start_char'] = ent.start_char\n",
    "\n",
    "            # get end char\n",
    "            result_dict['end_char'] = ent.end_char\n",
    "\n",
    "            # add result_dict to output_list\n",
    "            output_list.append(result_dict)\n",
    "            \n",
    "    # return output\n",
    "    return output_list\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_sent': 'Hello Cambridge University.',\n",
       "  '_sent_num': 1,\n",
       "  'end_char': 26,\n",
       "  'entity': 'Hello Cambridge University',\n",
       "  'start_char': 0,\n",
       "  'type': 'ORG'},\n",
       " {'_sent': 'How are you, Ryan?',\n",
       "  '_sent_num': 2,\n",
       "  'end_char': 17,\n",
       "  'entity': 'Ryan',\n",
       "  'start_char': 13,\n",
       "  'type': 'PERSON'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_spacy_string(\"Hello Cambridge University.\\n\\nHow are you, Ryan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_spacy(path_to_txt_file):\n",
    "    print(path_to_txt_file)\n",
    "    try:\n",
    "        with open(path_to_txt_file) as file:\n",
    "            txt=file.read()\n",
    "        return ner_spacy_string(txt)\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = ner_spacy_path('/Users/ryan/literarytextmining/corpora/fiction_since_1990/texts/Brown,_Dan.The_Da_Vinci_Code.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
